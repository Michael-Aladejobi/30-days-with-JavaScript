{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-Aladejobi/30-days-with-JavaScript/blob/main/Malwaredetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndnjpp7fItSE",
        "outputId": "906c5461-f3d0-4ca8-83e1-e1635166fd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhUzfTrAB9Gz",
        "outputId": "442798b0-fd57-4585-9da2-5854cdf00dd2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading dataset...\n",
            "Dataset loaded successfully.\n",
            "Dataset splits: dict_keys(['train', 'test', 'valid'])\n",
            "Train set size: 3728\n",
            "Feature: binary_name, Type: Value(dtype='string', id=None)\n",
            "Feature: labels, Type: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
            "Feature: functions, Type: Value(dtype='string', id=None)\n",
            "Creating datasets...\n",
            "Created training set with 2982 samples and validation set with 746 samples\n",
            "Creating model...\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  25%|██▌       | 47/187 [06:56<20:13,  8.66s/it, train_loss=0.128]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "ds = load_dataset(\"PurCL/malware-top-100\")\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Display dataset info\n",
        "print(f\"Dataset splits: {ds.keys()}\")\n",
        "print(f\"Train set size: {len(ds['train'])}\")\n",
        "for key in ds['train'].features.keys():\n",
        "    print(f\"Feature: {key}, Type: {ds['train'].features[key]}\")\n",
        "\n",
        "# Pre-process the dataset\n",
        "class MalwareDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, split='train'):\n",
        "        self.dataset = hf_dataset[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # Extract binary data (assuming it's a string of binary data)\n",
        "        binary_name = sample['binary_name']\n",
        "\n",
        "        # Convert binary string to fixed length vector (first 1024 bytes)\n",
        "        # This is a simplified approach - you might want to use more sophisticated feature extraction\n",
        "        binary_data = []\n",
        "        if isinstance(binary_name, str):\n",
        "            for char in binary_name[:1024]:\n",
        "                binary_data.append(ord(char) / 255.0)  # Normalize to [0,1]\n",
        "            # Pad if needed\n",
        "            binary_data = binary_data + [0] * (1024 - len(binary_data))\n",
        "        else:\n",
        "            binary_data = [0] * 1024\n",
        "\n",
        "        # Extract function data\n",
        "        functions = sample['functions']\n",
        "        if isinstance(functions, str):\n",
        "            # Simple feature extraction from functions\n",
        "            func_data = []\n",
        "            for char in functions[:1024]:\n",
        "                func_data.append(ord(char) / 255.0)\n",
        "            func_data = func_data + [0] * (1024 - len(func_data))\n",
        "        else:\n",
        "            func_data = [0] * 1024\n",
        "\n",
        "        # Extract label - convert to one-hot encoding\n",
        "        labels_list = sample['labels']\n",
        "        # Creating a multi-hot encoding for multi-label classification\n",
        "        labels = []\n",
        "        if isinstance(labels_list, list):\n",
        "            label_map = {\"stealer\": 0, \"startpage_\": 1, \"winnt1\": 2, \"zusy\": 3}\n",
        "            label_vector = [0] * len(label_map)\n",
        "            for label in labels_list:\n",
        "                if label in label_map:\n",
        "                    label_vector[label_map[label]] = 1\n",
        "            labels = label_vector\n",
        "        else:\n",
        "            labels = [0, 0, 0, 0]  # Default for missing labels\n",
        "\n",
        "        return {\n",
        "            'binary_data': torch.FloatTensor(binary_data),\n",
        "            'function_data': torch.FloatTensor(func_data),\n",
        "            'labels': torch.FloatTensor(labels)\n",
        "        }\n",
        "\n",
        "# Define the CNN model for binary data\n",
        "class BinaryDataCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryDataCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # Compute the correct FC input size\n",
        "        self.fc = nn.Linear(128 * 128, 128)  # Will calculate exact size in forward pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length]\n",
        "        x = x.unsqueeze(1)  # Add channel dimension [batch_size, 1, sequence_length]\n",
        "\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        # Flatten the tensor properly\n",
        "        batch_size = x.size(0)\n",
        "        x = x.reshape(batch_size, -1)  # Using reshape instead of view\n",
        "\n",
        "        # Adjust the FC layer on first forward pass if needed\n",
        "        if not hasattr(self, 'fc_size_set'):\n",
        "            input_size = x.size(1)\n",
        "            self.fc = nn.Linear(input_size, 128).to(x.device)\n",
        "            self.fc_size_set = True\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the Transformer model for function data\n",
        "class FunctionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FunctionTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(1, 64)  # Simple embedding layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        # We'll set the size of the FC layer in the first forward pass\n",
        "        self.fc = None\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, sequence_length]\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension [batch_size, sequence_length, 1]\n",
        "\n",
        "        x = self.embedding(x)  # [batch_size, sequence_length, 64]\n",
        "        x = self.transformer_encoder(x)  # [batch_size, sequence_length, 64]\n",
        "\n",
        "        # Reshape for pooling\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        emb_dim = x.size(2)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, 64, sequence_length]\n",
        "        x = self.pool(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Reshape correctly using reshape instead of view\n",
        "        x = x.reshape(batch_size, -1)  # Flatten\n",
        "\n",
        "        # Create the FC layer on first forward pass with correct input size\n",
        "        if self.fc is None:\n",
        "            input_size = x.size(1)\n",
        "            self.fc = nn.Linear(input_size, 128).to(x.device)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the ensemble model\n",
        "class BiModalEnsemble(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BiModalEnsemble, self).__init__()\n",
        "        self.binary_cnn = BinaryDataCNN()\n",
        "        self.function_transformer = FunctionTransformer()\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Linear(128 + 128, 128)\n",
        "        self.bn = nn.BatchNorm1d(128)\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, binary_data, function_data):\n",
        "        binary_features = self.binary_cnn(binary_data)\n",
        "        function_features = self.function_transformer(function_data)\n",
        "\n",
        "        # Feature fusion\n",
        "        combined = torch.cat((binary_features, function_features), dim=1)\n",
        "        combined = F.relu(self.bn(self.fusion(combined)))\n",
        "        combined = self.dropout(combined)\n",
        "        output = self.classifier(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create train and validation datasets\n",
        "def create_datasets():\n",
        "    full_dataset = MalwareDataset(ds)\n",
        "\n",
        "    # Calculate sizes\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    print(f\"Created training set with {train_size} samples and validation set with {val_size} samples\")\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
        "    criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for batch in progress_bar:\n",
        "            binary_data = batch['binary_data'].to(device)\n",
        "            function_data = batch['function_data'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(binary_data, function_data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'train_loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss, val_accuracy, val_preds, val_targets = evaluate_model(model, val_loader, criterion)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_malware_model.pth')\n",
        "            print(\"Model saved!\")\n",
        "\n",
        "    return history, val_preds, val_targets\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            binary_data = batch['binary_data'].to(device)\n",
        "            function_data = batch['function_data'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(binary_data, function_data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(data_loader)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    # Calculate accuracy for multi-label classification (exact match)\n",
        "    accuracy = np.mean(np.all(all_preds == all_targets, axis=1))\n",
        "\n",
        "    return val_loss, accuracy, all_preds, all_targets\n",
        "\n",
        "# Visualization functions\n",
        "def plot_confusion_matrices(preds, targets, class_names):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        cm = confusion_matrix(targets[:, i], preds[:, i])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])\n",
        "        axes[i].set_title(f'Confusion Matrix - {class_names[i]}')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png')\n",
        "    plt.close()\n",
        "\n",
        "    return 'confusion_matrices.png'\n",
        "\n",
        "def plot_correlation_matrix(targets, class_names):\n",
        "    corr_matrix = np.zeros((len(class_names), len(class_names)))\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            # Calculate correlation between labels\n",
        "            corr = np.corrcoef(targets[:, i], targets[:, j])[0, 1]\n",
        "            corr_matrix[i, j] = corr\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Label Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    return 'correlation_matrix.png'\n",
        "\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Time')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Time')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    return 'training_history.png'\n",
        "\n",
        "def generate_classification_report(preds, targets, class_names):\n",
        "    reports = {}\n",
        "\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        report = classification_report(targets[:, i], preds[:, i], output_dict=True)\n",
        "        reports[class_name] = report\n",
        "\n",
        "    # Overall metrics\n",
        "    accuracy = np.mean(np.all(preds == targets, axis=1))\n",
        "    hamming_loss = np.mean(np.mean(preds != targets, axis=1))\n",
        "\n",
        "    print(f\"Overall Accuracy (Exact Match): {accuracy:.4f}\")\n",
        "    print(f\"Hamming Loss: {hamming_loss:.4f}\")\n",
        "\n",
        "    for class_name, report in reports.items():\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(f\"Precision: {report['1']['precision']:.4f}\")\n",
        "        print(f\"Recall: {report['1']['recall']:.4f}\")\n",
        "        print(f\"F1-Score: {report['1']['f1-score']:.4f}\")\n",
        "\n",
        "    return reports\n",
        "\n",
        "# Add a testing function to evaluate on the test set\n",
        "def test_model(model_path, test_loader):\n",
        "    # Load the best model\n",
        "    model = BiModalEnsemble(num_classes=4).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    test_loss, test_accuracy, test_preds, test_targets = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return test_preds, test_targets\n",
        "\n",
        "def main():\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset, val_dataset = create_datasets()\n",
        "\n",
        "    # Create data loaders with smaller batch size to avoid memory issues\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    print(\"Creating model...\")\n",
        "    model = BiModalEnsemble(num_classes=4).to(device)\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    history, val_preds, val_targets = train_model(model, train_loader, val_loader, num_epochs=10)\n",
        "\n",
        "    # Class names for visualization\n",
        "    class_names = [\"stealer\", \"startpage_\", \"winnt1\", \"zusy\"]\n",
        "\n",
        "    print(\"Generating evaluation metrics...\")\n",
        "    # Plot confusion matrices\n",
        "    confusion_matrix_path = plot_confusion_matrices(val_preds, val_targets, class_names)\n",
        "    print(f\"Confusion matrices saved to: {confusion_matrix_path}\")\n",
        "\n",
        "    # Plot correlation matrix\n",
        "    correlation_matrix_path = plot_correlation_matrix(val_targets, class_names)\n",
        "    print(f\"Correlation matrix saved to: {correlation_matrix_path}\")\n",
        "\n",
        "    # Plot training history\n",
        "    history_plot_path = plot_training_history(history)\n",
        "    print(f\"Training history plot saved to: {history_plot_path}\")\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    reports = generate_classification_report(val_preds, val_targets, class_names)\n",
        "\n",
        "    # Create test dataset and test the model\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_dataset = MalwareDataset(ds, split='test')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "    test_preds, test_targets = test_model('best_malware_model.pth', test_loader)\n",
        "\n",
        "    # Generate test metrics\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    test_reports = generate_classification_report(test_preds, test_targets, class_names)\n",
        "\n",
        "    # Plot test confusion matrices\n",
        "    test_confusion_matrix_path = plot_confusion_matrices(test_preds, test_targets, class_names)\n",
        "    print(f\"Test confusion matrices saved to: {test_confusion_matrix_path}\")\n",
        "\n",
        "    print(\"\\nTraining and evaluation complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}